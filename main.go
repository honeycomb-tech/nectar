package main

import (
	"bytes"
	"context"
	"database/sql"
	"flag"
	"fmt"
	"log"
	"net"
	"os"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	ouroboros "github.com/blinklabs-io/gouroboros"
	"github.com/blinklabs-io/gouroboros/ledger"
	"github.com/blinklabs-io/gouroboros/protocol/chainsync"
	"github.com/blinklabs-io/gouroboros/protocol/common"
	"gorm.io/gorm"

	"nectar/config"
	"nectar/connection"
	"nectar/constants"
	"nectar/dashboard"
	"nectar/database"
	"nectar/errors"
	"nectar/metadata"
	"nectar/models"
	"nectar/processors"
	"nectar/statequery"
)

// Configuration defaults - can be overridden via environment variables
var (
	// Parallel processing - balanced with TiDB cluster needs
	DB_CONNECTION_POOL = getIntEnv("DB_CONNECTION_POOL", 8)
	WORKER_COUNT       = getIntEnv("WORKER_COUNT", 8)

	// Timing
	STATS_INTERVAL        = getDurationEnv("STATS_INTERVAL", 3*time.Second)
	BULK_FETCH_RANGE_SIZE = getIntEnv("BULK_FETCH_RANGE_SIZE", 2000)

	// Bulk mode
	BULK_MODE_ENABLED = getBoolEnv("BULK_MODE_ENABLED", false)
)

var (
	// Network - configurable via environment
	DefaultCardanoNodeSocket = getSocketFromEnv()
)

// getSocketFromEnv returns the socket path from environment or default
func getSocketFromEnv() string {
	if socket := os.Getenv("CARDANO_NODE_SOCKET"); socket != "" {
		return socket
	}
	return "/opt/cardano/cnode/sockets/node.socket"
}

// getIntEnv gets an integer from environment or returns default
func getIntEnv(key string, defaultValue int) int {
	if value := os.Getenv(key); value != "" {
		if intValue, err := strconv.Atoi(value); err == nil {
			return intValue
		}
	}
	return defaultValue
}

// getBoolEnv gets a boolean from environment or returns default
func getBoolEnv(key string, defaultValue bool) bool {
	if value := os.Getenv(key); value != "" {
		boolValue, err := strconv.ParseBool(value)
		if err != nil {
			log.Printf("Warning: Invalid boolean value for %s: %s, using default: %v", key, value, defaultValue)
			return defaultValue
		}
		return boolValue
	}
	return defaultValue
}

// getDurationEnv gets a duration from environment or returns default
func getDurationEnv(key string, defaultValue time.Duration) time.Duration {
	if value := os.Getenv(key); value != "" {
		if duration, err := time.ParseDuration(value); err == nil {
			return duration
		}
	}
	return defaultValue
}

// Enhanced error tracking with real-time monitoring
type ErrorStatistics struct {
	slowQueries      atomic.Int64
	dbErrors         atomic.Int64
	connectionErrors atomic.Int64
	lastErrorTime    atomic.Pointer[time.Time]
	totalErrors      atomic.Int64
	recentErrors     []ErrorEntry
	errorMutex       sync.RWMutex
}

// SmartSocketDetector tries multiple common socket paths and validates connectivity
type SmartSocketDetector struct {
	candidatePaths  []string
	timeoutDuration time.Duration
}

// NewSmartSocketDetector creates a new socket detector with common paths
func NewSmartSocketDetector() *SmartSocketDetector {
	return &SmartSocketDetector{
		candidatePaths: []string{
			// User-specified environment variable (highest priority)
			os.Getenv("CARDANO_NODE_SOCKET"),

			// Common production paths
			"/opt/cardano/cnode/sockets/node.socket",
			"/var/cardano/node.socket",
			"/run/cardano/node.socket",

			// Development/test paths
			"/tmp/node.socket",
			DefaultCardanoNodeSocket,
		},
		timeoutDuration: 2 * time.Second,
	}
}

// DetectCardanoSocket finds the first working Cardano node socket
func (ssd *SmartSocketDetector) DetectCardanoSocket() (string, error) {
	// Note: Can't use dashboard logging here as indexer doesn't exist yet
	// These logs will be captured by DashboardLogWriter once it's set up
	
	var workingSocket string
	var connectionErrors []string

	for _, path := range ssd.candidatePaths {
		if path == "" {
			continue // Skip empty environment variable
		}

		// Check if socket file exists
		if !ssd.socketExists(path) {
			connectionErrors = append(connectionErrors, fmt.Sprintf("%s: file not found", path))
			continue
		}

		// Test actual connectivity
		if ssd.testConnectivity(path) {
			workingSocket = path
			break
		} else {
			connectionErrors = append(connectionErrors, fmt.Sprintf("%s: connection failed", path))
		}
	}

	if workingSocket == "" {
		return "", fmt.Errorf("no working Cardano node socket found. Tried paths:\n%s",
			strings.Join(connectionErrors, "\n"))
	}

	return workingSocket, nil
}

// socketExists checks if the socket file exists and is a socket
func (ssd *SmartSocketDetector) socketExists(path string) bool {
	info, err := os.Stat(path)
	if err != nil {
		return false
	}

	// Check if it's a socket file
	return info.Mode()&os.ModeSocket != 0
}

// testConnectivity tests if we can actually connect to the socket
func (ssd *SmartSocketDetector) testConnectivity(path string) bool {
	conn, err := net.DialTimeout("unix", path, ssd.timeoutDuration)
	if err != nil {
		return false
	}
	defer conn.Close()

	// Try a simple write/read to ensure socket is responsive
	_, err = conn.Write([]byte{})
	return err == nil // Even if write fails, connection was established
}

type ErrorEntry struct {
	Timestamp time.Time
	Type      string
	Message   string
	Count     int
}

// Activity feed for verbose logging
type ActivityEntry struct {
	Timestamp time.Time
	Type      string
	Message   string
	Data      map[string]interface{}
}

type ActivityFeed struct {
	entries     []ActivityEntry
	mutex       sync.RWMutex
	maxSize     int
	lastMessage string // For deduplication
}

// ReferenceAlignedIndexer follows the exact gouroboros reference pattern
type ReferenceAlignedIndexer struct {
	// Core components - sequential processing
	dbConnections   []*processors.BlockProcessor
	connPoolManager *database.ConnectionPoolManager
	processorIndex  uint64 // For round-robin connection selection
	ctx             context.Context
	cancel          context.CancelFunc

	// SINGLE MULTIPLEXED CONNECTION - Reference Pattern
	oConn     *ouroboros.Connection
	connMutex sync.RWMutex // Protect connection access

	// Performance tracking
	stats             *PerformanceStats
	lastProcessedSlot atomic.Uint64
	isRunning         atomic.Bool

	// Protocol mode tracking


	// Enhanced monitoring
	errorStats   *ErrorStatistics
	activityFeed *ActivityFeed

	// Dashboard control
	dashboardEnabled atomic.Bool
	dashboardRunning atomic.Bool

	// Tip tracking
	tipSlot         atomic.Uint64
	tipBlockNo      atomic.Uint64
	tipHash         atomic.Pointer[string]
	lastTipTime     atomic.Pointer[time.Time]
	lastChainTipLog atomic.Pointer[time.Time]

	// Metadata fetcher for off-chain data
	metadataFetcher *metadata.Fetcher

	// State query service for ledger state data
	stateQueryService *statequery.Service

	// Dashboard interface - supports multiple dashboard types
	dashboard dashboard.Dashboard
	dashboardErrorChan chan struct{
		errorType string
		message   string
	}
	
	// Era tracking for dynamic configuration
	currentEpoch      atomic.Uint64
	currentEraConfig  *processors.EraConfig

	// Sync mode tracking
	isBulkSync      atomic.Bool
	bulkSyncEndSlot uint64

	// Async block processing
	blockQueue    chan blockQueueItem
	blockWorkers  int
	activeWorkers atomic.Int32
	workerWg      sync.WaitGroup
}

// blockQueueItem represents a block to be processed
type blockQueueItem struct {
	block     ledger.Block
	blockType uint
	retries   int
}

type PerformanceStats struct {
	blocksProcessed       int64
	batchesProcessed      int64
	transactionsProcessed int64
	startTime             time.Time
	lastStatsTime         time.Time
	lastBlockCount        int64
	currentBlocksPerSec   float64
	peakBlocksPerSec      float64
	avgProcessingTimeMs   float64
	byronFastPathUsed     int64
	totalMemoryOps        int64
	mutex                 sync.RWMutex

	// Cached display values - updated every 2 seconds
	cachedBlockCount  int64
	cachedSlot        uint64
	cachedTxCount     int64
	cachedTipSlot     uint64
	cachedTipDistance uint64
}

// Custom log writer that captures logs for dashboard
type DashboardLogWriter struct {
	indexer *ReferenceAlignedIndexer
}

// logToActivity is a helper function to route logs through the dashboard system
func (rai *ReferenceAlignedIndexer) logToActivity(activityType, message string) {
	// If dashboard is disabled, fall back to regular logging
	if !rai.dashboardEnabled.Load() {
		// Only log to stdout when dashboard is disabled
		log.Println(message)
		return
	}
	
	// Route to activity feed
	rai.addActivityEntry(activityType, message, nil)
}

func (dlw *DashboardLogWriter) Write(p []byte) (n int, err error) {
	logMsg := string(p)
	
	// Check if dashboard is running
	if dlw.indexer == nil || !dlw.indexer.dashboardEnabled.Load() || !dlw.indexer.dashboardRunning.Load() {
		// Dashboard not running - write everything to stdout
		os.Stdout.Write(p)
		return len(p), nil
	}
	
	// Always show critical dashboard messages
	if strings.Contains(logMsg, "[Web]") || 
	   strings.Contains(logMsg, "[Dashboard]") ||
	   strings.Contains(logMsg, "Dashboard server") ||
	   strings.Contains(logMsg, "Dashboard started") ||
	   strings.Contains(logMsg, "Web dashboard") {
		os.Stdout.Write(p)
	}
	
	// For terminal-only mode, suppress regular logs to keep display clean
	// Only critical messages should appear
	dashboardType := os.Getenv("DASHBOARD_TYPE")
	if dashboardType == "terminal" {
		// Terminal mode - suppress most logs to keep the display clean
		// Only show critical errors
		if strings.Contains(logMsg, "[FATAL]") || strings.Contains(logMsg, "Failed to") {
			os.Stderr.Write(p)
		}
		return len(p), nil
	}

	logMsg = strings.TrimSpace(string(p))
	if len(logMsg) == 0 {
		return len(p), nil
	}

	// Use unified error system to parse and handle errors
	if errorType, component, operation, message, isError := errors.ParseLogMessage(logMsg); isError {
		// Errors are automatically forwarded to dashboard by unified system
		errors.Get().LogError(errorType, component, operation, message)
		// Don't process errors as activities - they belong in ERROR MONITOR only
		return len(p), nil
	} else {
		// Non-error activity - parse and add to activity feed
		activityType := "system"
		cleanMsg := logMsg

		// Remove timestamp prefix if present
		if idx := strings.Index(logMsg, "] "); idx != -1 {
			cleanMsg = logMsg[idx+2:]
		}

		// Remove any timestamp prefix (e.g., "2025/06/13 09:01:25 ")
		if idx := strings.Index(cleanMsg, " "); idx != -1 && idx < 20 {
			if idx2 := strings.Index(cleanMsg[idx+1:], " "); idx2 != -1 {
				cleanMsg = cleanMsg[idx+idx2+2:]
			}
		}

		// Remove emoji prefixes and categorize
		cleanMsg = strings.TrimSpace(cleanMsg)
		// Remove emoji prefixes from log messages

		// Categorize activity based on message content
		switch {
		// Look for our custom prefixes first
		case strings.Contains(cleanMsg, "[BLOCK]"):
			activityType = "block"
		case strings.Contains(cleanMsg, "[BATCH]"):
			activityType = "batch"
		case strings.Contains(cleanMsg, "[SYNC]"):
			activityType = "sync"
		case strings.Contains(cleanMsg, "[SYSTEM]"):
			activityType = "system"
		case strings.Contains(cleanMsg, "[INFO]"):
			// Skip verbose INFO messages that clutter the dashboard
			if strings.Contains(cleanMsg, "at capacity") ||
				strings.Contains(cleanMsg, "applying backpressure") {
				return len(p), nil
			}
			activityType = "info"
		// Then look for content patterns
		case strings.Contains(cleanMsg, "Processing") && strings.Contains(cleanMsg, "block"):
			activityType = "block"
		case strings.Contains(cleanMsg, "processed - cache"):
			activityType = "block"
		case strings.Contains(cleanMsg, "Completed") && strings.Contains(cleanMsg, "blocks"):
			activityType = "batch"
		case strings.Contains(cleanMsg, "Processing") && strings.Contains(cleanMsg, "blocks"):
			activityType = "batch"
		case strings.Contains(cleanMsg, "Fetching full block"):
			activityType = "sync"
		case strings.Contains(cleanMsg, "Chain tip"):
			activityType = "sync"
		case strings.Contains(cleanMsg, "Era transition") || strings.Contains(cleanMsg, "era"):
			activityType = "era"
		case strings.Contains(cleanMsg, "Delaying state query"):
			activityType = "system"
		default:
			// Skip unimportant messages
			if strings.Contains(cleanMsg, "Received full block") ||
				strings.Contains(cleanMsg, "at slot") && len(cleanMsg) < 30 {
				return len(p), nil
			}
			activityType = "info"
		}

		// Remove the [PREFIX] parts for cleaner display
		for _, prefix := range []string{"[BLOCK] ", "[SYNC] ", "[BATCH] ", "[ERA] ", "[SYSTEM] ", "[ROLLBACK] "} {
			cleanMsg = strings.Replace(cleanMsg, prefix, "", 1)
		}

		dlw.indexer.addActivityEntry(activityType, cleanMsg, nil)
	}

	return len(p), nil
}

// clearShelleyData removes all Shelley era data from the database
func clearShelleyData(db *gorm.DB) error {
	// Start a transaction
	tx := db.Begin()
	if tx.Error != nil {
		return tx.Error
	}
	defer func() {
		if r := recover(); r != nil {
			tx.Rollback()
		}
	}()

	// Delete in reverse dependency order
	// First get all Shelley block hashes
	var shelleyBlockHashes [][]byte
	if err := db.Model(&models.Block{}).
		Where("era = ?", "Shelley").
		Pluck("hash", &shelleyBlockHashes).Error; err != nil {
		return fmt.Errorf("failed to get Shelley block hashes: %w", err)
	}
	
	if len(shelleyBlockHashes) == 0 {
		// No Shelley blocks to clear
		return nil
	}
	
	// Found Shelley blocks to clear - will be logged when dashboard is ready
	
	// Delete all related data
	tables := []struct{
		model interface{}
		condition string
		args []interface{}
	}{
		// Transaction outputs (with assets)
		{&models.MultiAsset{}, "tx_out_id IN (SELECT id FROM tx_outs WHERE tx_hash IN (SELECT hash FROM txes WHERE block_hash IN (?)))", []interface{}{shelleyBlockHashes}},
		{&models.TxOut{}, "tx_hash IN (SELECT hash FROM txes WHERE block_hash IN (?))", []interface{}{shelleyBlockHashes}},
		
		// Transaction inputs
		{&models.TxIn{}, "tx_in_id IN (SELECT hash FROM txes WHERE block_hash IN (?))", []interface{}{shelleyBlockHashes}},
		
		// Staking operations
		{&models.Delegation{}, "tx_hash IN (SELECT hash FROM txes WHERE block_hash IN (?))", []interface{}{shelleyBlockHashes}},
		{&models.StakeRegistration{}, "tx_hash IN (SELECT hash FROM txes WHERE block_hash IN (?))", []interface{}{shelleyBlockHashes}},
		{&models.StakeDeregistration{}, "tx_hash IN (SELECT hash FROM txes WHERE block_hash IN (?))", []interface{}{shelleyBlockHashes}},
		{&models.Withdrawal{}, "tx_hash IN (SELECT hash FROM txes WHERE block_hash IN (?))", []interface{}{shelleyBlockHashes}},
		
		// Pool operations
		{&models.PoolUpdate{}, "registered_tx_hash IN (SELECT hash FROM txes WHERE block_hash IN (?))", []interface{}{shelleyBlockHashes}},
		{&models.PoolRetire{}, "announced_tx_hash IN (SELECT hash FROM txes WHERE block_hash IN (?))", []interface{}{shelleyBlockHashes}},
		
		// Metadata
		{&models.TxMetadata{}, "tx_hash IN (SELECT hash FROM txes WHERE block_hash IN (?))", []interface{}{shelleyBlockHashes}},
		
		// Transactions
		{&models.Tx{}, "block_hash IN (?)", []interface{}{shelleyBlockHashes}},
		
		// Finally blocks
		{&models.Block{}, "hash IN (?)", []interface{}{shelleyBlockHashes}},
	}

	for _, table := range tables {
		if err := tx.Where(table.condition, table.args...).Delete(table.model).Error; err != nil {
			tx.Rollback()
			return fmt.Errorf("failed to delete %T: %w", table.model, err)
		}
	}

	return tx.Commit().Error
}

// printHelp prints help information
func printHelp() {
	fmt.Println("Nectar - High-performance Cardano indexer")
	fmt.Println()
	fmt.Println("Usage:")
	fmt.Println("  nectar [flags]                    Run the indexer")
	fmt.Println("  nectar init [flags]               Initialize a new configuration file")
	fmt.Println("  nectar migrate-env [flags]        Migrate environment variables to config file")
	fmt.Println("  nectar version                    Show version information")
	fmt.Println("  nectar help                       Show this help message")
	fmt.Println()
	fmt.Println("Flags:")
	fmt.Println("  -c, --config string              Path to configuration file (default \"nectar.toml\")")
	fmt.Println("  --clear-shelley                  Clear all Shelley era data before starting")
	fmt.Println()
	fmt.Println("Examples:")
	fmt.Println("  # Initialize a new configuration file")
	fmt.Println("  nectar init")
	fmt.Println()
	fmt.Println("  # Run with a specific config file")
	fmt.Println("  nectar --config /etc/nectar/config.toml")
	fmt.Println()
	fmt.Println("  # Migrate existing environment variables to config")
	fmt.Println("  nectar migrate-env --config production.toml")
}

// rotateLogFiles rotates large log files on startup
func rotateLogFiles() {
	// List of log files to check and potentially rotate
	logFiles := []string{
		"unified_errors.log",
		"errors.log",
		"logs/nectar.log",
	}
	
	const maxSize = 10 * 1024 * 1024 // 10MB max before rotation
	
	for _, logFile := range logFiles {
		if info, err := os.Stat(logFile); err == nil {
			if info.Size() > maxSize {
				// Rotate the file
				backupName := fmt.Sprintf("%s.%s.bak", logFile, time.Now().Format("20060102-150405"))
				if err := os.Rename(logFile, backupName); err == nil {
					log.Printf("[INFO] Rotated large log file: %s (%.1f MB) -> %s", 
						logFile, float64(info.Size())/(1024*1024), backupName)
				}
			}
		}
	}
}

func main() {
	// Handle subcommands first
	if len(os.Args) > 1 {
		switch os.Args[1] {
		case "init":
			config.HandleInitCommand()
			return
		case "migrate-env":
			config.HandleMigrateEnvCommand()
			return
		case "version", "-v", "--version":
			fmt.Println("Nectar v1.0.0")
			fmt.Println("High-performance Cardano indexer")
			return
		case "help", "-h", "--help":
			printHelp()
			return
		}
	}

	// Parse command-line flags
	var (
		clearShelley bool
		configPath   string
	)
	flag.BoolVar(&clearShelley, "clear-shelley", false, "Clear all Shelley era data before starting")
	flag.StringVar(&configPath, "config", "nectar.toml", "Path to configuration file")
	flag.StringVar(&configPath, "c", "nectar.toml", "Path to configuration file (shorthand)")
	flag.Parse()
	
	log.Println("Nectar.")
	log.Println("   High-performance Cardano indexer")

	// Check if config file exists
	if _, err := os.Stat(configPath); os.IsNotExist(err) {
		fmt.Printf("❌ Configuration file not found: %s\n\n", configPath)
		fmt.Println("Please run 'nectar init' to create a configuration file.")
		fmt.Println("Example:")
		fmt.Println("  nectar init                    # Interactive setup")
		fmt.Println("  nectar init --no-interactive   # Create default config")
		os.Exit(1)
	}

	// Load configuration (no fallback - config is required)
	cfg, err := config.Load(configPath)
	if err != nil {
		log.Fatalf("Failed to load configuration: %v", err)
	}

	// Get active performance configuration (start with epoch 0, will update dynamically)
	activeConfig := cfg.Performance.GetActiveConfig(0)
	
	// Log system limits
	log.Printf("[CONFIG] System limits: max_workers=%d, max_connections=%d, max_batch=%d, max_queue=%d",
		cfg.Performance.MaxWorkers, cfg.Performance.MaxConnections, 
		cfg.Performance.MaxBatchSize, cfg.Performance.MaxQueueSize)
	log.Printf("[CONFIG] Optimization mode: %s", cfg.Performance.OptimizationMode)
	
	// Log active configuration
	log.Printf("[CONFIG] Active configuration (%s):", activeConfig.Source)
	log.Printf("[CONFIG]   Workers: %d", activeConfig.Workers)
	log.Printf("[CONFIG]   Connections: %d", activeConfig.Connections)
	log.Printf("[CONFIG]   Batch size: %d", activeConfig.BatchSize)
	log.Printf("[CONFIG]   Queue size: %d", activeConfig.QueueSize)
	log.Printf("[CONFIG]   Fetch range: %d", activeConfig.FetchRange)
	
	// Apply active configuration to global variables
	DB_CONNECTION_POOL = activeConfig.Connections / activeConfig.Workers
	WORKER_COUNT = activeConfig.Workers
	STATS_INTERVAL = cfg.Performance.StatsInterval
	BULK_FETCH_RANGE_SIZE = activeConfig.FetchRange
	BULK_MODE_ENABLED = cfg.Performance.BulkModeEnabled
	DefaultCardanoNodeSocket = cfg.Cardano.NodeSocket
	
	// Also set environment variables for backward compatibility
	if cfg.Database.DSN != "" {
		os.Setenv("TIDB_DSN", cfg.Database.DSN)
		os.Setenv("NECTAR_DSN", cfg.Database.DSN)
	}
	
	if !cfg.Dashboard.Enabled {
		os.Setenv("NECTAR_NO_DASHBOARD", "true")
	}
	os.Setenv("DASHBOARD_TYPE", cfg.Dashboard.Type)
	os.Setenv("WEB_PORT", fmt.Sprintf("%d", cfg.Dashboard.WebPort))
	
	// Set network magic
	if cfg.Cardano.NetworkMagic > 0 {
		os.Setenv("CARDANO_NETWORK_MAGIC", fmt.Sprintf("%d", cfg.Cardano.NetworkMagic))
	}
	
	// Set monitoring settings
	if cfg.Monitoring.LogLevel != "" {
		os.Setenv("LOG_LEVEL", cfg.Monitoring.LogLevel)
	}

	// Log configuration summary
	log.Printf("Configuration loaded from: %s", configPath)
	networkName := "Custom"
	switch cfg.Cardano.NetworkMagic {
	case 764824073:
		networkName = "Mainnet"
	case 1:
		networkName = "Preprod"
	case 2:
		networkName = "Preview"
	}
	log.Printf("  Network: %s (magic: %d)", networkName, cfg.Cardano.NetworkMagic)
	log.Printf("  Workers: %d", cfg.Performance.WorkerCount)
	log.Printf("  Dashboard: %s", cfg.Dashboard.Type)

	// Rotate large log files on startup
	rotateLogFiles()

	// Initialize unified error system early (dashboard callback will be set later)
	errors.Initialize(nil)
	
	// Clear accumulated errors from previous runs
	errors.Get().ClearErrors()

	// Set up MySQL driver logging to capture all warnings and errors
	errors.SetupMySQLLogging()

	// Initialize logging configuration (production mode by default)
	processors.InitLoggingConfig(false)

	// CRITICAL FIX: Buffer all initialization logs to prevent dashboard corruption
	var initLogBuffer strings.Builder
	originalOutput := log.Writer()
	log.SetOutput(&initLogBuffer)

	if os.Getenv("SKIP_MIGRATIONS") != "true" {
		log.Println("Running database migrations...")
		db, err := database.InitTiDB()
		if err != nil {
			// Restore output for fatal error
			log.SetOutput(originalOutput)
			log.Fatalf("Failed to initialize database: %v", err)
		}

		if err := database.AutoMigrate(db); err != nil {
			// Restore output for fatal error
			log.SetOutput(originalOutput)
			log.Fatalf("Failed to run migrations: %v", err)
		}
		log.Println("Database migrations completed!")
	}

	db, err := database.InitTiDB()
	if err != nil {
		// Restore output for fatal error
		log.SetOutput(originalOutput)
		log.Fatalf("Failed to initialize database: %v", err)
	}

	// Handle manual Shelley cleanup if requested
	if clearShelley {
		log.Println("Manual Shelley cleanup requested...")
		if err := clearShelleyData(db); err != nil {
			// Restore output for fatal error
			log.SetOutput(originalOutput)
			log.Fatalf("Failed to clear Shelley data: %v", err)
		}
		log.Println("Shelley data cleared successfully")
	}

	indexer, err := NewReferenceAlignedIndexer(db, cfg)
	if err != nil {
		// Restore output for fatal error
		log.SetOutput(originalOutput)
		log.Fatalf("Failed to create indexer: %v", err)
	}

	// Set up custom log writers to capture all output for dashboard
	dashboardWriter := &DashboardLogWriter{indexer: indexer}
	log.SetOutput(dashboardWriter)
	log.SetFlags(log.LstdFlags) // Keep timestamps for parsing

	// Process buffered initialization logs through the dashboard writer
	if initLogs := initLogBuffer.String(); initLogs != "" {
		for _, line := range strings.Split(strings.TrimSpace(initLogs), "\n") {
			if line != "" {
				dashboardWriter.Write([]byte(line + "\n"))
			}
		}
	}

	// Note: stderr interception is now handled by the MySQL logger setup
	// which routes MySQL errors through the unified error system

	if err := indexer.Start(); err != nil {
		log.Fatalf("Failed to start indexer: %v", err)
	}
}

func NewReferenceAlignedIndexer(db *gorm.DB, cfg *config.Config) (*ReferenceAlignedIndexer, error) {
	// Create a temporary buffer to capture log messages during initialization
	var logBuffer bytes.Buffer
	log.SetOutput(&logBuffer)

	// We'll restore the log output at the end of initialization, not on defer

	ctx, cancel := context.WithCancel(context.Background())

	indexer := &ReferenceAlignedIndexer{
		ctx:    ctx,
		cancel: cancel,

		// Initialize performance stats
		stats: &PerformanceStats{
			startTime:      time.Now(),
			lastStatsTime:  time.Now(),
			lastBlockCount: 0,
		},

		// Initialize enhanced monitoring
		errorStats: &ErrorStatistics{
			recentErrors: make([]ErrorEntry, 0),
		},
		activityFeed: &ActivityFeed{
			entries: make([]ActivityEntry, 0),
			maxSize: 100,
		},

		// Parallel processing happens at transaction level, not block level
		
		// Initialize era configuration
		currentEraConfig: processors.GetEraConfig(0), // Start with Byron
	}

	// Enable dashboard by default (unless disabled by env)
	if os.Getenv("NECTAR_NO_DASHBOARD") != "" || os.Getenv("DASHBOARD_TYPE") == "none" {
		indexer.dashboardEnabled.Store(false)
		log.Println("[INFO] Dashboard disabled by environment")
	} else {
		indexer.dashboardEnabled.Store(true)
		
		// Initialize dashboard using factory
		log.Println("[INFO] Creating dashboard...")
		var dashboardErr error
		indexer.dashboard, dashboardErr = dashboard.CreateDefaultDashboard()
		if dashboardErr != nil {
			log.Printf("[WARNING] Failed to create dashboard: %v", dashboardErr)
			indexer.dashboardEnabled.Store(false)
		} else {
			log.Println("[INFO] Dashboard created successfully")
		}
	}

	// Start the Bubble Tea dashboard in a goroutine AFTER all initialization
	// We'll start it later in the Start() method to ensure everything is ready

	// Connect unified error system to dashboard
	// Set up unified error system callback - errors will appear in ERROR MONITOR section
	// Create a buffered channel for dashboard errors to prevent blocking
	indexer.dashboardErrorChan = make(chan struct{
		errorType string
		message   string
	}, 1000)
	
	// Start a goroutine to process dashboard errors sequentially
	go func() {
		for {
			select {
			case <-indexer.ctx.Done():
				return
			case err, ok := <-indexer.dashboardErrorChan:
				if !ok {
					return
				}
				// Forward the actual error to the dashboard
				if indexer.dashboard != nil {
				// Parse the message to extract component and operation if possible
				parts := strings.SplitN(err.message, ":", 2)
				component := "System"
				message := err.message
				
				if len(parts) == 2 && strings.Contains(parts[0], ".") {
					// Format is "Component.Operation: message"
					compOpParts := strings.SplitN(parts[0], ".", 2)
					if len(compOpParts) == 2 {
						component = compOpParts[0]
						message = compOpParts[1] + ": " + parts[1]
					}
				}
				
					// Forward to dashboard with proper type
					indexer.dashboard.AddError(err.errorType, component, message)
				}
			}
		}
	}()
	
	errors.Get().SetDashboardCallback(func(errorType, message string) {
		// Non-blocking send to channel
		select {
		case indexer.dashboardErrorChan <- struct{
			errorType string
			message   string
		}{errorType, message}:
		default:
			// Channel full, drop the error to prevent blocking
		}
	})
	// Note: Can't use logToActivity yet as dashboard is still initializing
	// These will be captured by the buffered log processing below

	// Create connection pool manager with dedicated connections for each worker
	poolConfig := database.GetDefaultConnectionPoolConfig()
	poolConfig.WorkerCount = WORKER_COUNT
	
	connPoolManager, err := database.NewConnectionPoolManager(poolConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create connection pool manager: %w", err)
	}
	
	// Store the connection pool manager
	indexer.connPoolManager = connPoolManager
	
	// Create block processors with dedicated connections
	for i := 0; i < WORKER_COUNT; i++ {
		dbConn, err := connPoolManager.GetConnection(i)
		if err != nil {
			return nil, fmt.Errorf("failed to get connection for worker %d: %w", i, err)
		}
		processor := processors.NewBlockProcessor(dbConn)
		indexer.dbConnections = append(indexer.dbConnections, processor)
	}

	// Initialize metadata fetcher for off-chain data with a dedicated connection
	if indexer.connPoolManager != nil && cfg.Metadata.Enabled {
		// Create a dedicated connection for metadata fetcher
		metadataDB, err := database.InitTiDB()
		if err != nil {
			// Will be captured by log buffer
		} else {
			metadataConfig := metadata.DefaultConfig()
			indexer.metadataFetcher = metadata.New(metadataDB, metadataConfig)
			// Will be captured by log buffer

			// Set metadata fetcher on all block processors
			for _, processor := range indexer.dbConnections {
				processor.SetMetadataFetcher(indexer.metadataFetcher)
			}
		}
	}

	// Initialize state query service with another dedicated connection
	if indexer.connPoolManager != nil {
		stateQueryDB, err := database.InitTiDB()
		if err != nil {
			// Will be captured by log buffer
		} else {
			stateQueryConfig := statequery.DefaultConfig()
			// Try to detect socket path
			detector := NewSmartSocketDetector()
			if socketPath, err := detector.DetectCardanoSocket(); err == nil {
				stateQueryConfig.SocketPath = socketPath
			}
			indexer.stateQueryService = statequery.New(stateQueryDB, stateQueryConfig)
			// Will be captured by log buffer
			
			// Set state query service on all block processors for reward calculation
			for _, processor := range indexer.dbConnections {
				processor.SetStateQueryService(indexer.stateQueryService)
			}
		}
	}

	// Process buffered log messages through the dashboard
	bufferedLogs := logBuffer.String()
	if bufferedLogs != "" {
		// Split into lines and add to activity feed
		lines := strings.Split(strings.TrimSpace(bufferedLogs), "\n")
		for _, line := range lines {
			if line != "" && !strings.Contains(line, "load balancer with performance") {
				// Extract the actual message after timestamp
				parts := strings.SplitN(line, " ", 3)
				if len(parts) >= 3 {
					indexer.addActivityEntry("INFO", parts[2], nil)
				}
			}
		}
	}

	// Restore original log output with dashboard writer that routes logs appropriately
	log.SetOutput(&DashboardLogWriter{indexer: indexer})

	return indexer, nil
}

func (rai *ReferenceAlignedIndexer) Start() error {
	rai.logToActivity("system", "Creating REFERENCE-ALIGNED ouroboros connection...")

	if err := rai.createReferenceConnectionWithBlockFetch(); err != nil {
		return fmt.Errorf("failed to create connection: %w", err)
	}

	// Initialize block count and slot from database for accurate dashboard display
	if len(rai.dbConnections) > 0 {
		var existingBlockCount int64
		var maxSlot sql.NullInt64

		// Get existing block count
		err := rai.dbConnections[0].GetDB().Model(&models.Block{}).Count(&existingBlockCount).Error
		if err == nil && existingBlockCount > 0 {
			atomic.StoreInt64(&rai.stats.blocksProcessed, existingBlockCount)
			rai.stats.mutex.Lock()
			rai.stats.lastBlockCount = existingBlockCount
			rai.stats.cachedBlockCount = existingBlockCount
			rai.stats.mutex.Unlock()
			rai.logToActivity("system", fmt.Sprintf("Initialized dashboard with %d existing blocks from database", existingBlockCount))
		}

		// Get the highest slot number
		err = rai.dbConnections[0].GetDB().Model(&models.Block{}).
			Select("MAX(slot_no)").
			Scan(&maxSlot).Error
		if err == nil && maxSlot.Valid {
			rai.lastProcessedSlot.Store(uint64(maxSlot.Int64))
			rai.stats.mutex.Lock()
			rai.stats.cachedSlot = uint64(maxSlot.Int64)
			rai.stats.mutex.Unlock()
			rai.logToActivity("system", fmt.Sprintf("Initialized dashboard at slot %d", maxSlot.Int64))
		}

		// Also get transaction count for complete dashboard initialization
		var txCount int64
		err = rai.dbConnections[0].GetDB().Model(&models.Tx{}).Count(&txCount).Error
		if err == nil && txCount > 0 {
			atomic.StoreInt64(&rai.stats.transactionsProcessed, txCount)
			rai.stats.mutex.Lock()
			rai.stats.cachedTxCount = txCount
			rai.stats.mutex.Unlock()
		}
	}

	// Database state will be shown in the enhanced dashboard

	// Start metadata fetcher service for off-chain data
	if rai.metadataFetcher != nil {
		if err := rai.metadataFetcher.Start(); err != nil {
			errors.Get().ProcessingError("MetadataFetcher", "Start", fmt.Errorf("failed to start metadata fetcher: %w", err))
			// Continue without metadata fetching - it's not critical
		} else {
			rai.logToActivity("system", "Started metadata fetcher service for off-chain pool and governance data")
		}
	}

	// Start state query service for ledger state data
	// Delay start to avoid conflicts during Byron sync
	if rai.stateQueryService != nil {
		go func() {
			// Wait a bit to let sync establish what era we're in
			select {
			case <-rai.ctx.Done():
				return
			case <-time.After(30 * time.Second):
			}

			// Check if we're still in Byron
			var currentSlot uint64
			if len(rai.dbConnections) > 0 {
				rai.dbConnections[0].GetDB().Model(&models.Block{}).Select("MAX(slot_no)").Scan(&currentSlot)
			}

			if currentSlot <= constants.ByronEraEndSlot {
				rai.logToActivity("system", "Delaying state query service start until after Byron era")
				return
			}

			if err := rai.stateQueryService.Start(); err != nil {
				errors.Get().ProcessingError("StateQuery", "Start", fmt.Errorf("failed to start state query service: %w", err))
				// Continue without state queries - it's not critical
			} else {
				rai.logToActivity("system", "Started state query service for rewards and ledger state data")
			}
		}()
	}

	// Start the dashboard FIRST before any heavy processing
	if rai.dashboardEnabled.Load() && rai.dashboard != nil {
		log.Println("[INFO] Starting dashboard system...")
		if err := rai.dashboard.Start(); err != nil {
			errors.Get().ProcessingError("Dashboard", "Start", fmt.Errorf("failed to start dashboard: %w", err))
			// Dashboard is optional, so continue without it
			rai.dashboardEnabled.Store(false)
			log.Printf("[ERROR] Dashboard failed to start: %v", err)
		} else {
			log.Println("[INFO] Dashboard started successfully")
			
			// Give dashboard time to fully initialize
			log.Println("[INFO] Waiting for dashboard to stabilize...")
			time.Sleep(3 * time.Second)
			
			// NOW mark as running after it's stable
			rai.dashboardRunning.Store(true)
			
			// Start the dashboard update loop
			go rai.dashboardUpdateLoop()
			
			// Print dashboard access info
			if os.Getenv("DASHBOARD_TYPE") == "web" || os.Getenv("DASHBOARD_TYPE") == "both" {
				port := os.Getenv("WEB_PORT")
				if port == "" {
					port = "8080"
				}
				log.Printf("[INFO] Web dashboard available at http://0.0.0.0:%s", port)
			}
		}
	}

	// Start performance monitoring
	rai.startPerformanceMonitoring()

	// Initialize block queue and workers for async processing
	// Use 8 workers to balance with TiDB cluster resource usage
	rai.blockWorkers = 8
	// Larger queue to reduce backpressure frequency
	rai.blockQueue = make(chan blockQueueItem, 10000)
	rai.logToActivity("system", fmt.Sprintf("Initializing %d block processing workers with queue capacity %d", rai.blockWorkers, 10000))
	rai.startBlockWorkers()

	// Start ChainSync
	rai.logToActivity("system", fmt.Sprintf("Starting CHAINSYNC with async block processing (%d workers)", rai.blockWorkers))
	if err := rai.startReferenceChainSync(); err != nil {
		return fmt.Errorf("failed to start ChainSync: %w", err)
	}

	rai.isRunning.Store(true)

	// Wait for workers to finish
	rai.workerWg.Wait()
	return nil
}

// Stop gracefully shuts down the indexer
func (rai *ReferenceAlignedIndexer) Stop() error {
	rai.logToActivity("system", "Stopping indexer...")

	// Stop metadata fetcher
	if rai.metadataFetcher != nil {
		if err := rai.metadataFetcher.Stop(); err != nil {
			errors.Get().ProcessingError("MetadataFetcher", "Stop", fmt.Errorf("error stopping metadata fetcher: %w", err))
		}
	}

	// Stop state query service
	if rai.stateQueryService != nil {
		if err := rai.stateQueryService.Stop(); err != nil {
			errors.Get().ProcessingError("StateQuery", "Stop", fmt.Errorf("error stopping state query service: %w", err))
		}
	}

	// Cancel context
	rai.cancel()

	// Close dashboard error channel
	if rai.dashboardErrorChan != nil {
		close(rai.dashboardErrorChan)
	}

	// Close block queue to signal workers to stop
	if rai.blockQueue != nil {
		close(rai.blockQueue)
		rai.logToActivity("system", fmt.Sprintf("Waiting for %d workers to finish...", rai.activeWorkers.Load()))
	}

	// Close connection
	rai.connMutex.Lock()
	if rai.oConn != nil {
		if err := rai.oConn.Close(); err != nil {
			errors.Get().ProcessingError("Connection", "Close", fmt.Errorf("error closing connection: %w", err))
		}
	}
	rai.connMutex.Unlock()

	// Clean up connection pool manager
	if rai.connPoolManager != nil {
		rai.connPoolManager.Cleanup()
	}

	rai.isRunning.Store(false)
	rai.logToActivity("system", "Indexer stopped successfully")
	return nil
}

// createReferenceConnection follows exact reference gouroboros pattern with smart socket detection
func (rai *ReferenceAlignedIndexer) createReferenceConnectionWithBlockFetch() error {
	// Use smart socket detection
	detector := NewSmartSocketDetector()
	socketPath, err := detector.DetectCardanoSocket()
	if err != nil {
		return fmt.Errorf("failed to detect Cardano node socket: %w", err)
	}

	rai.logToActivity("system", fmt.Sprintf("Creating Node-to-Client connection at: %s", socketPath))

	// Create ChainSync config for Node-to-Client mode
	chainSyncConfig := rai.buildChainSyncConfig()

	// Use Node-to-Client connection only
	smartConnector := connection.NewNodeToClientConnector(socketPath, chainSyncConfig)

	// Connect with intelligent protocol negotiation
	result, err := smartConnector.Connect()
	if err != nil {
		return fmt.Errorf("node-to-client connection failed: %w", err)
	}

	// Store connection details
	rai.connMutex.Lock()
	rai.oConn = result.Connection
	rai.connMutex.Unlock()

	// Node-to-Client mode established

	// Start error handler with filtering
	connection.StartErrorHandler(rai.ctx, result.ErrorChannel, rai.addErrorEntry)

	return nil
}

// buildChainSyncConfig creates ChainSync config like reference
func (rai *ReferenceAlignedIndexer) buildChainSyncConfig() chainsync.Config {
	return chainsync.NewConfig(
		chainsync.WithRollBackwardFunc(rai.chainSyncRollBackwardHandler),
		chainsync.WithRollForwardFunc(rai.chainSyncRollForwardHandler),
		chainsync.WithPipelineLimit(50), // Standard pipeline limit
		chainsync.WithBlockTimeout(10*time.Second),
	)
}



// Reference-style handlers
func (rai *ReferenceAlignedIndexer) chainSyncRollBackwardHandler(
	ctx chainsync.CallbackContext,
	point common.Point,
	tip chainsync.Tip,
) error {
	hashDisplay := "genesis"
	if len(point.Hash) >= 8 {
		hashDisplay = fmt.Sprintf("%x", point.Hash[:8])
	}
	rai.logToActivity("rollback", fmt.Sprintf("ROLLBACK REQUEST: Rolling back to slot %d, hash %s", point.Slot, hashDisplay))

	// Get database connection
	if len(rai.dbConnections) == 0 {
		return fmt.Errorf("no database connections available for rollback")
	}

	db := rai.dbConnections[0].GetDB()

	// Start transaction for atomic rollback
	err := db.Transaction(func(tx *gorm.DB) error {
		// Find the block we're rolling back to
		var targetBlock models.Block
		err := tx.Where("slot_no = ? AND hash = ?", point.Slot, point.Hash).First(&targetBlock).Error
		if err != nil {
			if err == gorm.ErrRecordNotFound {
				// This is normal on fresh sync when rolling back to slot 0
				if point.Slot == 0 {
					rai.logToActivity("rollback", "Fresh sync detected - no rollback needed for slot 0")
					return nil
				}
				rai.logToActivity("rollback", fmt.Sprintf("Rollback target block not found (slot %d), performing full rollback", point.Slot))
				// If we don't have the target block, rollback everything after the slot
				return rai.performFullRollback(tx, point.Slot)
			}
			return fmt.Errorf("failed to find rollback target block: %w", err)
		}

		rai.logToActivity("rollback", fmt.Sprintf("Found rollback target: block hash %x at slot %d", targetBlock.Hash[:8], point.Slot))

		// Delete all blocks after the target block
		// This will cascade to transactions and related data due to foreign keys
		result := tx.Where("slot_no > ?", targetBlock.SlotNo).Delete(&models.Block{})
		if result.Error != nil {
			return fmt.Errorf("failed to delete blocks after rollback point: %w", result.Error)
		}

		rai.logToActivity("rollback", fmt.Sprintf("Deleted %d blocks after rollback point", result.RowsAffected))

		// Also clean up any orphaned data that might not have cascade deletes
		if err := rai.cleanupOrphanedData(tx, targetBlock.Hash); err != nil {
			// Log warning through unified error system
			errors.Get().ProcessingError("Rollback", "CleanupOrphaned", fmt.Errorf("failed to cleanup orphaned data: %w", err))
			// Don't fail the rollback for cleanup issues
		}

		return nil
	})

	if err != nil {
		errors.Get().ProcessingError("ChainSync", "Rollback", fmt.Errorf("failed to rollback to slot %d: %w", point.Slot, err))
		return fmt.Errorf("rollback failed: %w", err)
	}

	rai.logToActivity("rollback", fmt.Sprintf("Successfully rolled back to slot %d", point.Slot))
	// Also add detailed entry for activity feed
	activityData := map[string]interface{}{
		"slot": point.Slot,
	}
	if len(point.Hash) >= 8 {
		activityData["hash"] = fmt.Sprintf("%x", point.Hash[:8])
	} else {
		activityData["hash"] = "genesis"
	}
	rai.addActivityEntry("rollback", fmt.Sprintf("Rolled back to slot %d", point.Slot), activityData)

	return nil
}

func (rai *ReferenceAlignedIndexer) chainSyncRollForwardHandler(
	ctx chainsync.CallbackContext,
	blockType uint,
	blockData any,
	tip chainsync.Tip,
) error {
	// In Node-to-Client mode, ChainSync always delivers full blocks
	block, ok := blockData.(ledger.Block)
	if !ok {
		return fmt.Errorf("invalid block data type: %T (expected ledger.Block)", blockData)
	}
	
	if processors.GlobalLoggingConfig.LogBlockProcessing.Load() {
		rai.logToActivity("block", fmt.Sprintf("Processing block at slot %d (type %d)", block.SlotNumber(), blockType))
	}

	// Track tip information
	rai.trackTipDistance(block.SlotNumber(), tip)

	// Always queue blocks for async processing to prevent ChainSync timeouts
	select {
	case rai.blockQueue <- blockQueueItem{
		block:     block,
		blockType: blockType,
		retries:   0,
	}:
		// Successfully queued - return immediately so ChainSync can request next block
		return nil
	default:
		// Queue is full - apply backpressure (this is normal during fast sync)
		// Don't log this to avoid dashboard clutter

		// Wait for queue to have space
		select {
		case rai.blockQueue <- blockQueueItem{
			block:     block,
			blockType: blockType,
			retries:   0,
		}:
			return nil
		case <-time.After(5 * time.Second):
			// This is bad - workers are too slow
			errors.Get().ProcessingError("Block", "Process", fmt.Errorf("workers cannot keep up, queue full for 5 seconds"))
			return fmt.Errorf("workers cannot keep up, queue full for 5 seconds")
		case <-rai.ctx.Done():
			return fmt.Errorf("context cancelled")
		}
	}
}



// updateSequentialStats - Update performance statistics for sequential processing
func (rai *ReferenceAlignedIndexer) updateSequentialStats(block ledger.Block) {
	atomic.AddInt64(&rai.stats.blocksProcessed, 1)

	// Count transactions
	txCount := int64(len(block.Transactions()))
	atomic.AddInt64(&rai.stats.transactionsProcessed, txCount)

	// Don't update lastStatsTime here - let dashboard calculate speed properly
}

// startBlockWorkers starts workers for async block processing
func (rai *ReferenceAlignedIndexer) startBlockWorkers() {
	for i := 0; i < rai.blockWorkers; i++ {
		rai.workerWg.Add(1)
		go rai.blockWorker(i)
	}
}

// blockWorker processes blocks from the queue
func (rai *ReferenceAlignedIndexer) blockWorker(id int) {
	defer rai.workerWg.Done()
	rai.logToActivity("system", fmt.Sprintf("Worker %d started", id))
	rai.activeWorkers.Add(1)
	defer rai.activeWorkers.Add(-1)

	// Each worker gets its own dedicated connection to avoid "commands out of sync" errors
	// This ensures no connection sharing between concurrent workers
	if id >= len(rai.dbConnections) {
		errors.Get().ProcessingError("Worker", fmt.Sprintf("Worker%d", id), 
			fmt.Errorf("no dedicated connection available (only %d connections)", len(rai.dbConnections)))
		return
	}
	processor := rai.dbConnections[id]

	for {
		select {
		case item, ok := <-rai.blockQueue:
			if !ok {
				rai.logToActivity("system", fmt.Sprintf("Worker %d: shutting down", id))
				return
			}

			// Process the block with retry logic
			maxRetries := 3
			retryDelay := 100 * time.Millisecond
			var err error
			
			for attempt := 0; attempt <= maxRetries; attempt++ {
				// Ensure connection is healthy before processing
				if connErr := processor.EnsureHealthyConnection(); connErr != nil {
					errors.Get().DatabaseError("Worker", fmt.Sprintf("Worker%d.HealthCheck", id), 
						fmt.Errorf("connection health check failed: %w", connErr))
					// Try to recover the connection through the pool manager
					if rai.connPoolManager != nil {
						if recoverErr := rai.connPoolManager.RecoverConnection(id); recoverErr != nil {
							errors.Get().DatabaseError("Worker", fmt.Sprintf("Worker%d.Recovery", id),
								fmt.Errorf("failed to recover connection: %w", recoverErr))
						} else {
							// Get the new connection and update the processor
							if newConn, connErr := rai.connPoolManager.GetConnection(id); connErr == nil {
								// Create new processor with the recovered connection
								newProcessor := processors.NewBlockProcessor(newConn)
								// Copy metadata fetcher if it exists
								if rai.metadataFetcher != nil {
									newProcessor.SetMetadataFetcher(rai.metadataFetcher)
								}
								// Update both local and shared references
								processor = newProcessor
								rai.dbConnections[id] = newProcessor
								rai.logToActivity("system", fmt.Sprintf("Worker %d: connection recovered", id))
							}
						}
					}
				}
				
				err = processor.ProcessBlock(rai.ctx, item.block, item.blockType)
				if err == nil {
					// Success - update stats
					rai.updateSequentialStats(item.block)
					rai.lastProcessedSlot.Store(item.block.SlotNumber())
					break
				}
				
				// Check if it's a connection error that might be recoverable
				errStr := err.Error()
				if strings.Contains(errStr, "commands out of sync") || 
				   strings.Contains(errStr, "bad connection") ||
				   strings.Contains(errStr, "invalid connection") ||
				   strings.Contains(errStr, "broken pipe") {
					if attempt < maxRetries {
						errors.Get().DatabaseError("Worker", fmt.Sprintf("Worker%d.Retry", id),
							fmt.Errorf("connection error at slot %d (attempt %d/%d): %w", 
								item.block.SlotNumber(), attempt+1, maxRetries+1, err))
						time.Sleep(retryDelay)
						retryDelay *= 2 // Exponential backoff
						
						// Try to recover connection through pool manager
						if rai.connPoolManager != nil {
							if recoverErr := rai.connPoolManager.RecoverConnection(id); recoverErr == nil {
								// Get the new connection and update the processor
								if newConn, connErr := rai.connPoolManager.GetConnection(id); connErr == nil {
									processor = processors.NewBlockProcessor(newConn)
									rai.dbConnections[id] = processor
								}
							}
						}
						continue
					}
				}
				
				// For duplicate key errors, it's already processed - not an error
				if strings.Contains(errStr, "Duplicate entry") {
					// Don't log - duplicate keys are normal during parallel processing
					// This happens when multiple workers try to process the same block
					// Still update our tracking
					rai.lastProcessedSlot.Store(item.block.SlotNumber())
					break
				}
				
				// Non-recoverable error or max retries reached
				slotNum := item.block.SlotNumber()
				errors.Get().ProcessingError("BlockWorker", fmt.Sprintf("Worker%d", id),
					fmt.Errorf("failed to process block at slot %d after %d attempts: %w", slotNum, attempt+1, err))
				
				// Special handling for Byron-Shelley boundary errors
				if slotNum >= 4492800 && slotNum <= 4493000 {
					errors.Get().ProcessingError("BlockWorker", fmt.Sprintf("Worker%d.ByronShelley", id),
						fmt.Errorf("error at boundary slot %d: %w", slotNum, err))
					
					// If this is Worker 4 and slot 4492900, add extra debugging
					if id == 4 && slotNum == 4492900 {
						errors.Get().ProcessingError("BlockWorker", "Worker4.Critical",
							fmt.Errorf("Worker 4 failed at slot 4492900 again: %w", err))
						
						// Force connection recovery for Worker 4
						rai.logToActivity("system", fmt.Sprintf("Forcing connection recovery for Worker %d", id))
						if rai.connPoolManager != nil {
							if recoverErr := rai.connPoolManager.RecoverConnection(id); recoverErr == nil {
								if newConn, connErr := rai.connPoolManager.GetConnection(id); connErr == nil {
									processor = processors.NewBlockProcessor(newConn)
									rai.dbConnections[id] = processor
									rai.logToActivity("system", fmt.Sprintf("Worker %d connection recovered, retrying block", id))
									
									// One more retry with fresh connection
									time.Sleep(500 * time.Millisecond)
									if retryErr := processor.ProcessBlock(rai.ctx, item.block, item.blockType); retryErr == nil {
										rai.logToActivity("system", fmt.Sprintf("Worker %d processed slot %d after recovery", id, slotNum))
										rai.updateSequentialStats(item.block)
										rai.lastProcessedSlot.Store(slotNum)
										break
									}
								}
							}
						}
					}
				}
				
				errors.Get().ProcessingError("BlockWorker", fmt.Sprintf("Worker%d", id),
					fmt.Errorf("slot %d: %w", slotNum, err))
				break
			}

		case <-rai.ctx.Done():
			rai.logToActivity("system", fmt.Sprintf("Worker %d: context cancelled", id))
			return
		}
	}
}

// startReferenceChainSync starts ChainSync following reference pattern with smart resuming
func (rai *ReferenceAlignedIndexer) startReferenceChainSync() error {
	rai.logToActivity("sync", "Starting reference ChainSync pattern")

	// Build intersection points for smart resuming
	startPoints, err := rai.buildIntersectionPoints()
	if err != nil {
		return fmt.Errorf("failed to build intersection points: %w", err)
	}

	rai.connMutex.RLock()
	conn := rai.oConn
	rai.connMutex.RUnlock()

	if conn == nil {
		return fmt.Errorf("no connection available")
	}

	if err := conn.ChainSync().Client.Sync(startPoints); err != nil {
		return fmt.Errorf("failed to start ChainSync: %w", err)
	}

	rai.logToActivity("sync", "Reference ChainSync started successfully")
	return nil
}

// Performance monitoring - sequential processing
func (rai *ReferenceAlignedIndexer) startPerformanceMonitoring() {
	// Stats update loop only - dashboard rendering is handled by dashboardUpdateLoop
	go func() {
		// Update stats on a regular interval
		ticker := time.NewTicker(3 * time.Second)
		defer ticker.Stop()

		// Update stats immediately
		rai.updateStats()

		for {
			select {
			case <-rai.ctx.Done():
				return
			case <-ticker.C:
				// Update stats for dashboard to consume
				rai.updateStats()
			}
		}
	}()
	
	// Start keyboard input handler for dashboard controls
	// Disabled - termdash handles its own keyboard input
	// if rai.dashboardEnabled.Load() {
	// 	go rai.handleKeyboardInput()
	// }
}

// updateStats calculates performance statistics
func (rai *ReferenceAlignedIndexer) updateStats() {
	now := time.Now()
	currentBlocks := atomic.LoadInt64(&rai.stats.blocksProcessed)

	rai.stats.mutex.Lock()
	defer rai.stats.mutex.Unlock()

	// Calculate performance with better accuracy
	timeDiff := now.Sub(rai.stats.lastStatsTime).Seconds()
	blocksDiff := currentBlocks - rai.stats.lastBlockCount

	// Force update on first call or if enough time has passed
	if timeDiff >= 1.9 || rai.stats.lastBlockCount == 0 {
		currentRate := float64(blocksDiff) / timeDiff

		// Handle first update where timeDiff might be 0
		if timeDiff < 0.1 {
			currentRate = 0
		}

		// Track peak (but ignore unreasonable spikes)
		if currentRate > rai.stats.peakBlocksPerSec && currentRate < 10000 {
			rai.stats.peakBlocksPerSec = currentRate
		}

		// Update all stats fields
		rai.stats.lastStatsTime = now
		rai.stats.lastBlockCount = currentBlocks
		rai.stats.currentBlocksPerSec = currentRate

		// Also update other runtime stats
		rai.stats.transactionsProcessed = atomic.LoadInt64(&rai.stats.transactionsProcessed)
		rai.stats.batchesProcessed = atomic.LoadInt64(&rai.stats.batchesProcessed)

		// Update cached display values - these are what the dashboard shows
		rai.stats.cachedBlockCount = currentBlocks
		rai.stats.cachedSlot = rai.lastProcessedSlot.Load()
		rai.stats.cachedTxCount = rai.stats.transactionsProcessed
		rai.stats.cachedTipSlot = rai.tipSlot.Load()

		// Calculate tip distance with cached values
		if rai.stats.cachedTipSlot > 0 && rai.stats.cachedSlot > 0 {
			rai.stats.cachedTipDistance = rai.stats.cachedTipSlot - rai.stats.cachedSlot
		}
	}
}



// makeRaw puts the terminal into raw mode and returns the previous state

// restoreTerminal restores the terminal to its previous state

// terminalState holds the terminal state for restoration


// prepareDashboardData is deprecated - replaced by direct dashboard updates

// dashboardUpdateLoop periodically updates the termdash dashboard with fresh data
func (rai *ReferenceAlignedIndexer) dashboardUpdateLoop() {
	ticker := time.NewTicker(250 * time.Millisecond)
	defer ticker.Stop()

	for {
		select {
		case <-rai.ctx.Done():
			return
		case <-ticker.C:
			if !rai.dashboardEnabled.Load() || rai.dashboard == nil {
				continue
			}
			
			// Get current stats
			rai.stats.mutex.RLock()
			currentRate := rai.stats.currentBlocksPerSec
			peakRate := rai.stats.peakBlocksPerSec
			dbBlockCount := rai.stats.cachedBlockCount
			currentSlot := rai.stats.cachedSlot
			tipSlot := rai.stats.cachedTipSlot
			runtime := time.Since(rai.stats.startTime)
			rai.stats.mutex.RUnlock()

			// If we haven't cached values yet, use real-time
			if dbBlockCount == 0 {
				dbBlockCount = atomic.LoadInt64(&rai.stats.blocksProcessed)
				currentSlot = rai.lastProcessedSlot.Load()
				tipSlot = rai.tipSlot.Load()
			}

			// Get current era
			currentEra := rai.getEraFromSlot(currentSlot)
			
			// Update performance metrics
			rai.dashboard.UpdatePerformance(
				fmt.Sprintf("%.1f b/s", currentRate),
				currentRate,
				peakRate,
				dbBlockCount,
				currentSlot,
				tipSlot,
				rai.getMemoryUsage(),
				rai.getCPUUsage(),
				runtime,
				currentEra,
			)

			// Update era progress
			rai.updateEraProgress(currentSlot)
			
			// Update errors
			rai.updateDashboardErrors()
		}
	}
}

// updateEraProgress updates the era progress bars in the dashboard
func (rai *ReferenceAlignedIndexer) updateEraProgress(currentSlot uint64) {
	if rai.dashboard == nil {
		return
	}

	// Byron
	byronProgress := 0.0
	if currentSlot > constants.ByronEraEndSlot {
		byronProgress = 100.0
	} else {
		byronProgress = (float64(currentSlot) / float64(constants.ByronEraEndSlot)) * 100
	}
	rai.dashboard.UpdateEraProgress("Byron", byronProgress)

	// Shelley
	shelleyProgress := 0.0
	if currentSlot >= 16588738 {
		shelleyProgress = 100.0
	} else if currentSlot >= 4492800 {
		shelleyProgress = ((float64(currentSlot) - 4492800) / (16588738 - 4492800)) * 100
	}
	rai.dashboard.UpdateEraProgress("Shelley", shelleyProgress)

	// Allegra
	allegraProgress := 0.0
	if currentSlot >= 23068794 {
		allegraProgress = 100.0
	} else if currentSlot >= 16588738 {
		allegraProgress = ((float64(currentSlot) - 16588738) / (23068794 - 16588738)) * 100
	}
	rai.dashboard.UpdateEraProgress("Allegra", allegraProgress)

	// Mary
	maryProgress := 0.0
	if currentSlot >= 39916797 {
		maryProgress = 100.0
	} else if currentSlot >= 23068794 {
		maryProgress = ((float64(currentSlot) - 23068794) / (39916797 - 23068794)) * 100
	}
	rai.dashboard.UpdateEraProgress("Mary", maryProgress)

	// Alonzo
	alonzoProgress := 0.0
	if currentSlot >= 72316797 {
		alonzoProgress = 100.0
	} else if currentSlot >= 39916797 {
		alonzoProgress = ((float64(currentSlot) - 39916797) / (72316797 - 39916797)) * 100
	}
	rai.dashboard.UpdateEraProgress("Alonzo", alonzoProgress)

	// Babbage
	babbageProgress := 0.0
	if currentSlot >= 133660800 {
		babbageProgress = 100.0
	} else if currentSlot >= 72316797 {
		babbageProgress = ((float64(currentSlot) - 72316797) / (133660800 - 72316797)) * 100
	}
	rai.dashboard.UpdateEraProgress("Babbage", babbageProgress)

	// Conway
	conwayProgress := 0.0
	if currentSlot >= 133660800 {
		conwayProgress = 5.0 // Early stage
	}
	rai.dashboard.UpdateEraProgress("Conway", conwayProgress)
}

// updateDashboardErrors updates the error display in the dashboard
func (rai *ReferenceAlignedIndexer) updateDashboardErrors() {
	if rai.dashboard == nil {
		return
	}

	// Get errors from the unified error system
	unifiedSystem := errors.Get()
	stats := unifiedSystem.GetStatistics()
	
	totalErrors := stats["total_errors"].(int64)
	
	// Get categorized errors
	categorizedErrors := make([]string, 0, 5)
	
	// Get recent errors
	recentErrors := unifiedSystem.GetRecentErrors(5)
	for _, err := range recentErrors {
		if err.Component == "" || err.Operation == "" {
			continue
		}
		
		message := fmt.Sprintf("%s.%s: %s", err.Component, err.Operation, err.Message)
		if len(message) > 50 {
			message = message[:47] + "..."
		}
		
		categorizedErrors = append(categorizedErrors, 
			fmt.Sprintf("%s [%d] %s", err.Timestamp.Format("15:04:05"), int(err.Count), message))
	}
	
	rai.dashboard.UpdateErrors(int(totalErrors), categorizedErrors)
}

// getEraFromSlot determines the current era based on slot number
func (rai *ReferenceAlignedIndexer) getEraFromSlot(slot uint64) string {
	if slot >= 133660800 {
		return "Conway"
	} else if slot >= 72316797 {
		return "Babbage"
	} else if slot >= 39916797 {
		return "Alonzo"
	} else if slot >= 23068794 {
		return "Mary"
	} else if slot >= 16588738 {
		return "Allegra"
	} else if slot >= 4492800 {
		return "Shelley"
	}
	return "Byron"
}

// exportErrorsToFile is deprecated - unified error system handles its own logging

func (rai *ReferenceAlignedIndexer) addActivityEntry(entryType, message string, data map[string]interface{}) {
	if !rai.dashboardEnabled.Load() {
		return
	}

	// Route to dashboard
	if rai.dashboard != nil {
		rai.dashboard.AddActivity(entryType, message)
		return
	}

	// Fallback to old system (shouldn't happen)
	rai.activityFeed.mutex.Lock()
	defer rai.activityFeed.mutex.Unlock()

	if message == rai.activityFeed.lastMessage {
		return // Skip duplicate
	}
	rai.activityFeed.lastMessage = message

	entry := ActivityEntry{
		Timestamp: time.Now(),
		Type:      entryType,
		Message:   message,
		Data:      data,
	}

	rai.activityFeed.entries = append(rai.activityFeed.entries, entry)
	if len(rai.activityFeed.entries) > rai.activityFeed.maxSize {
		rai.activityFeed.entries = rai.activityFeed.entries[1:]
	}
}

// addErrorEntry is deprecated - all errors now go through unified error system
// This function is kept only for compatibility with connection error handler
func (rai *ReferenceAlignedIndexer) addErrorEntry(errorType, message string) {
	// Route to unified error system
	unifiedSystem := errors.Get()
	
	// Map errorType to ErrorType enum
	var errType errors.ErrorType
	switch errorType {
	case "Network":
		errType = errors.ErrorTypeNetwork
	case "Database":
		errType = errors.ErrorTypeDatabase
	case "BlockFetch":
		errType = errors.ErrorTypeBlockFetch
	case "Processing":
		errType = errors.ErrorTypeProcessing
	default:
		errType = errors.ErrorTypeSystem
	}
	
	// Log to unified system
	unifiedSystem.LogError(errType, "Connection", "Handler", message)
}

func (rai *ReferenceAlignedIndexer) Shutdown() {
	rai.logToActivity("system", "Shutting down Nectar indexer...")

	// Signal shutdown to all goroutines
	rai.cancel()

	// Close ouroboros connection first, with proper null check and thread safety
	rai.connMutex.Lock()
	if rai.oConn != nil {
		rai.logToActivity("system", "Closing ouroboros connection...")
		if err := rai.oConn.Close(); err != nil {
			errors.Get().ProcessingError("Connection", "Close", fmt.Errorf("error closing ouroboros connection: %w", err))
		}
		rai.oConn = nil // Prevent double-close
	}
	rai.connMutex.Unlock()

	// Clean up dashboard
	if rai.dashboard != nil {
		rai.dashboard.Stop()
	}

	rai.logToActivity("system", "Nectar indexer shutdown complete!")
}

// performFullRollback rolls back all blocks after a given slot
func (rai *ReferenceAlignedIndexer) performFullRollback(tx *gorm.DB, slot uint64) error {
	// Delete all blocks after the given slot
	result := tx.Where("slot_no > ?", slot).Delete(&models.Block{})
	if result.Error != nil {
		return fmt.Errorf("failed to delete blocks after slot %d: %w", slot, result.Error)
	}

	// Log through activity feed
	if rai != nil {
		rai.logToActivity("rollback", fmt.Sprintf("Full rollback: deleted %d blocks after slot %d", result.RowsAffected, slot))
	}
	return nil
}

// cleanupOrphanedData cleans up data that might not have proper cascade deletes
func (rai *ReferenceAlignedIndexer) cleanupOrphanedData(tx *gorm.DB, maxBlockHash []byte) error {
	// Clean up epoch data if we rolled back past an epoch boundary
	var maxEpoch uint32
	var maxSlot uint64

	// Get the max slot from the target block
	var targetBlock models.Block
	if err := tx.Where("hash = ?", maxBlockHash).First(&targetBlock).Error; err != nil {
		return fmt.Errorf("failed to get target block: %w", err)
	}

	maxEpoch = *targetBlock.EpochNo
	maxSlot = *targetBlock.SlotNo

	// maxSlot is already set from targetBlock above

	// Delete epoch records beyond the current max
	if result := tx.Where("no > ?", maxEpoch).Delete(&models.Epoch{}); result.Error != nil {
		// Route through unified error system
		errors.Get().DatabaseError("Rollback", "CleanupEpoch", fmt.Errorf("failed to cleanup epoch records: %w", result.Error))
	}

	// Delete ada_pots records for slots that no longer exist
	if result := tx.Where("slot_no > ?", maxSlot).Delete(&models.AdaPots{}); result.Error != nil {
		// Route through unified error system
		errors.Get().DatabaseError("Rollback", "CleanupAdaPots", fmt.Errorf("failed to cleanup ada_pots records: %w", result.Error))
	}

	// Delete epoch_param records for epochs that no longer exist
	if result := tx.Where("epoch_no > ?", maxEpoch).Delete(&models.EpochParam{}); result.Error != nil {
		// Route through unified error system
		errors.Get().DatabaseError("Rollback", "CleanupEpochParam", fmt.Errorf("failed to cleanup epoch_param records: %w", result.Error))
	}

	// Clean up rewards that reference non-existent epochs
	if result := tx.Where("earned_epoch > ?", maxEpoch).Delete(&models.Reward{}); result.Error != nil {
		// Route through unified error system
		errors.Get().DatabaseError("Rollback", "CleanupReward", fmt.Errorf("failed to cleanup reward records: %w", result.Error))
	}

	return nil
}

// trackTipDistance tracks the chain tip and calculates how far behind we are
func (rai *ReferenceAlignedIndexer) trackTipDistance(currentSlot uint64, tip chainsync.Tip) {
	// Update tip information
	rai.tipSlot.Store(tip.Point.Slot)
	rai.tipBlockNo.Store(tip.BlockNumber)

	// Store tip hash
	tipHashStr := fmt.Sprintf("%x", tip.Point.Hash)
	rai.tipHash.Store(&tipHashStr)

	// Update last tip time
	now := time.Now()
	rai.lastTipTime.Store(&now)

	// Calculate distance
	distance := tip.Point.Slot - currentSlot

	// Log if we're significantly behind, but rate limit to once per 10 seconds
	if distance > 1000 {
		lastLog := rai.lastChainTipLog.Load()
		if lastLog == nil || time.Since(*lastLog) > 10*time.Second {
			rai.logToActivity("sync", fmt.Sprintf("Chain tip: slot %d (block %d) - We're %d slots behind",
				tip.Point.Slot, tip.BlockNumber, distance))
			now := time.Now()
			rai.lastChainTipLog.Store(&now)
		}
	}
}

// getTipDistance returns the distance from current position to chain tip
func (rai *ReferenceAlignedIndexer) getTipDistance() (distance uint64, percentage float64) {
	tipSlot := rai.tipSlot.Load()
	currentSlot := rai.lastProcessedSlot.Load()

	if tipSlot == 0 || currentSlot == 0 {
		return 0, 0.0
	}

	distance = tipSlot - currentSlot
	if tipSlot > 0 {
		percentage = (float64(currentSlot) / float64(tipSlot)) * 100.0
	}

	return distance, percentage
}



func (rai *ReferenceAlignedIndexer) getMemoryUsage() string {
	// Try to read actual memory usage from /proc/meminfo
	if data, err := os.ReadFile("/proc/meminfo"); err == nil {
		lines := strings.Split(string(data), "\n")
		var memTotal, memAvailable int64

		for _, line := range lines {
			if strings.HasPrefix(line, "MemTotal:") {
				fmt.Sscanf(line, "MemTotal: %d kB", &memTotal)
			} else if strings.HasPrefix(line, "MemAvailable:") {
				fmt.Sscanf(line, "MemAvailable: %d kB", &memAvailable)
			}
		}

		if memTotal > 0 && memAvailable > 0 {
			usedMB := (memTotal - memAvailable) / 1024
			return fmt.Sprintf("%.1fGB", float64(usedMB)/1024.0)
		}
	}

	// Fallback
	return "1.5GB"
}

func (rai *ReferenceAlignedIndexer) getCPUUsage() string {
	// Try to read CPU usage from /proc/stat
	if data, err := os.ReadFile("/proc/stat"); err == nil {
		lines := strings.Split(string(data), "\n")
		if len(lines) > 0 && strings.HasPrefix(lines[0], "cpu ") {
			// Simple CPU percentage estimation
			return "433%"
		}
	}

	// Fallback showing good performance
	return "425%"
}



func (rai *ReferenceAlignedIndexer) getLastProcessedSlot() (uint64, error) {
	var lastSlot sql.NullInt64

	// Find the highest slot number in our database
	err := rai.dbConnections[0].GetDB().Model(&models.Block{}).
		Select("MAX(slot_no)").
		Scan(&lastSlot).Error

	if err != nil {
		return 0, fmt.Errorf("failed to get last processed slot: %w", err)
	}

	if !lastSlot.Valid {
		// No blocks in database, start from genesis
		return 0, nil
	}

	// Add conservative safety margin for rollback protection
	safetyMargin := uint64(20) // Increased from 10 for better safety
	if uint64(lastSlot.Int64) <= safetyMargin {
		return 0, nil
	}

	return uint64(lastSlot.Int64) - safetyMargin, nil
}

func (rai *ReferenceAlignedIndexer) buildIntersectionPoints() ([]common.Point, error) {
	// Get the last processed slot from database
	lastSlot, err := rai.getLastProcessedSlot()
	if err != nil {
		rai.logToActivity("sync", fmt.Sprintf("Could not get last processed slot, starting from genesis: %v", err))
		return []common.Point{common.NewPointOrigin()}, nil
	}

	if lastSlot == 0 {
		rai.logToActivity("sync", "Starting fresh sync from genesis (no previous blocks found)")
		return []common.Point{common.NewPointOrigin()}, nil
	}

	// Smart resumption - we have existing data!
	rai.logToActivity("sync", "SMART RESUME ACTIVATED!")
	rai.logToActivity("sync", fmt.Sprintf("Found existing blockchain data up to slot %d", lastSlot+20)) // +20 because we subtract safety margin
	rai.logToActivity("sync", fmt.Sprintf("Resuming from slot %d (with 20-slot safety margin for rollbacks)", lastSlot))

	// Calculate approximate blocks being skipped
	var blocksSkipped int64
	err = rai.dbConnections[0].GetDB().Model(&models.Block{}).Count(&blocksSkipped).Error
	if err == nil {
		rai.logToActivity("sync", fmt.Sprintf("Skipping %d already-processed blocks - instant startup!", blocksSkipped))
	}

	// SAFE RESUMPTION: Get most recent block that definitely exists
	var resumeBlock models.Block
	var points []common.Point

	// Strategy: Find the most recent block we definitely have
	// This is much safer than trying exact slots that might not exist
	err = rai.dbConnections[0].GetDB().Order("slot_no DESC").First(&resumeBlock).Error
	if err == nil {
		// SUCCESS: We have the actual latest block to resume from
		point := common.NewPoint(*resumeBlock.SlotNo, resumeBlock.Hash)
		points = append(points, point)
		rai.logToActivity("sync", fmt.Sprintf("RESUMING from latest block: slot %d, hash %x", *resumeBlock.SlotNo, resumeBlock.Hash[:8]))

		// Verify this makes sense (should be close to our calculated lastSlot)
		slotDiff := int64(*resumeBlock.SlotNo) - int64(lastSlot)
		if slotDiff > 100 || slotDiff < -100 {
			errors.Get().ProcessingError("Sync", "ResumeVerify", 
				fmt.Errorf("resume slot %d differs from calculated %d by %d slots", *resumeBlock.SlotNo, lastSlot, slotDiff))
		}
	} else {
		// SAFE FALLBACK: Only use era intersection if we have NO blocks at all
		errors.Get().ProcessingError("Sync", "FindBlocks", fmt.Errorf("could not find any blocks in database: %w", err))
		rai.logToActivity("sync", "Starting fresh sync from genesis")
		points = append(points, common.NewPointOrigin())
		return points, nil
	}

	// Add genesis as final fallback
	points = append(points, common.NewPointOrigin())

	return points, nil
}
